{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d59ef08-45ac-4c87-bcf3-b831d3832377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chess\n",
    "import chess.pgn\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee71b31-1450-43da-90cc-6980a11cbded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fen_to_numeric(char):\n",
    "    \n",
    "    # Switches fen characters to numeric piece values\n",
    "    # White pieces are positive numbers and black pieces are negative numbers\n",
    "    #\n",
    "    # argument : A non-numeric character in a FEN string\n",
    "    # return   : Numeric value of input piece\n",
    "    \n",
    "    switcher = {\n",
    "        'r' : -5,\n",
    "        'n' : -3,\n",
    "        'b' : -4,\n",
    "        'q' : -9,\n",
    "        'k' : -10,\n",
    "        'p' : -1,\n",
    "        'R' : 5,\n",
    "        'N' : 3,\n",
    "        'B' : 4,\n",
    "        'Q' : 9,\n",
    "        'K' : 10,\n",
    "        'P' : 1,\n",
    "    }\n",
    "    return(switcher.get(char))\n",
    "\n",
    "def get_board_state(board): \n",
    "    \n",
    "    # Takes a chess board and converts the PGN to FEN notation. Then subs the \n",
    "    # FEN characters for the value of each piece and splits the result into an array\n",
    "    #\n",
    "    # argument : chess board object\n",
    "    # return   : input array for NN\n",
    "\n",
    "    shredder_fen = list((board.shredder_fen().split(\" \")[0]).replace(\"/\",\"\"))\n",
    "    numeric_fen = []\n",
    "    for char in shredder_fen:\n",
    "        if(char.isdigit()): # empty squares between pieces: add zeros\n",
    "            for i in range(int(char)):\n",
    "                numeric_fen.append(0)\n",
    "        else: # convert piece notation to numeric value\n",
    "            numeric_fen.append(fen_to_numeric(char))\n",
    "    \n",
    "    # Whose turn is it?\n",
    "    if(board.turn == True):\n",
    "        numeric_fen + [1] # White to move\n",
    "    else:\n",
    "        numeric_fen + [0] # Black to move\n",
    "    \n",
    "    # Is the king in check?\n",
    "    if(board.is_check() == True):\n",
    "        numeric_fen += [1] # Yes\n",
    "    else:\n",
    "        numeric_fen += [0] # No\n",
    "    \n",
    "    # Is queenside castling available for player?\n",
    "    if(board.has_queenside_castling_rights(board.turn)):\n",
    "        numeric_fen += [1] # Yes\n",
    "    else:\n",
    "        numeric_fen += [0] # No\n",
    "        \n",
    "    # Is kingside castling available for player?\n",
    "    if(board.has_kingside_castling_rights(board.turn)):\n",
    "        numeric_fen += [1] # Yes\n",
    "    else:\n",
    "        numeric_fen += [0] # No\n",
    "        \n",
    "    # Is queenside castling available for opponent?\n",
    "    if(board.has_queenside_castling_rights(not(board.turn))):\n",
    "        numeric_fen += [1] # Yes\n",
    "    else:\n",
    "        numeric_fen += [0] # No\n",
    "        \n",
    "    # Is kingside castling available for player?\n",
    "    if(board.has_kingside_castling_rights(not(board.turn))):\n",
    "        numeric_fen += [1] # Yes\n",
    "    else:\n",
    "        numeric_fen += [0] # No\n",
    "        \n",
    "    # How many turns have passed\n",
    "    numeric_fen += [board.ply()]\n",
    "\n",
    "    return(numeric_fen)\n",
    "\n",
    "    # Who won?\n",
    "#     if(board.has_kingside_castling_rights(not(board.turn))):\n",
    "#         numeric_fen += [1] # Yes\n",
    "#     else:\n",
    "#         numeric_fen += [0] # No\n",
    "\n",
    "#     return(numeric_fen)\n",
    "    \n",
    "\n",
    "def game_to_data(game, color, period):\n",
    "    \n",
    "    # Takes a chess game and processes it into a inputs to the NN\n",
    "    # The board state is a numeric representation generated by get_board_state()\n",
    "    # target_origin move is the space from which a piece is moved: target for first NN\n",
    "    # target_dest move is the space to which a piece is moved: target for second NN\n",
    "    #\n",
    "    # argument : chess game object\n",
    "    # return   : [flattened board states, target origin moves, target destination moves]\n",
    "    \n",
    "    # determine upper and lower bound to separate early, mid, and late game\n",
    "    if period == 'early':\n",
    "        lb = 0\n",
    "        ub = 14\n",
    "    elif period == 'mid':\n",
    "        lb = 15\n",
    "        ub = 40\n",
    "    else:\n",
    "        lb = 41\n",
    "        ub = np.inf\n",
    "    \n",
    "    \n",
    "    board_state = []\n",
    "    target_origin = []\n",
    "    target_dest = []\n",
    "    board = game.board()\n",
    "    \n",
    "    # Who won?\n",
    "    if(game.headers['Result'] == \"1-0\"):\n",
    "        winner = [1]\n",
    "    elif(game.headers['Result'] == \"0-1\"):\n",
    "        winner = [-1]\n",
    "    else:\n",
    "        winner = [0]\n",
    "    \n",
    "    for move in game.mainline_moves():\n",
    "        if(board.turn == color and lb <= board.ply() <= ub):\n",
    "            board_state.append(get_board_state(board) + winner) # add the board state\n",
    "            target_origin.append(move.from_square)\n",
    "            target_dest.append(move.to_square)\n",
    "        board.push(move) # play the next move on the board\n",
    "    \n",
    "    return board_state, target_origin, target_dest\n",
    "\n",
    "def aggregate_game_data(game1, game2):\n",
    "    \n",
    "    # Combines game data \n",
    "    #\n",
    "    # argument : chess game object\n",
    "    # return   : [flattened board states, target origin moves, target destination moves]\n",
    "    \n",
    "    X1 = game1[0]\n",
    "    X2 = game2[0]\n",
    "    y1 = game1[1]\n",
    "    y2 = game2[1]\n",
    "    z1 = game1[2]\n",
    "    z2 = game2[2]\n",
    "\n",
    "    return X1 + X2, y1 + y2, z1 + z2\n",
    "\n",
    "def conv_nn_data(pgn_games, color, period):\n",
    "    \n",
    "    # Wrapper for the other functions. Goes through the list of games and converts the\n",
    "    # games to training and target data\n",
    "    #\n",
    "    # argument : list of chess game objects\n",
    "    # return   : [flattened board states, target origin moves, target destination moves]\n",
    "\n",
    "    nn_data = game_to_data(pgn_games[0], color, period)\n",
    "    pgn_games = pgn_games[1:]\n",
    "    \n",
    "    for game in pgn_games:\n",
    "        nn_data = aggregate_game_data(nn_data, game_to_data(game, color, period))\n",
    "    \n",
    "    return np.array(nn_data[0]), np.array(nn_data[1]), np.array(nn_data[2]) # inputs, target1, target2\n",
    "\n",
    "def read_pgn_from_file(file):\n",
    "    \n",
    "    # Reads in all of the games in PGN form from a .pgn file and saves them as a list\n",
    "    #\n",
    "    # argument : .pgn file path\n",
    "    # return   : list of chess game objects\n",
    "    \n",
    "    pgn = open(file)\n",
    "    pgn_games = []\n",
    "    while(True):\n",
    "        game = chess.pgn.read_game(pgn)\n",
    "        if(game == None):\n",
    "            break\n",
    "        pgn_games.append(game)\n",
    "    return(pgn_games)\n",
    "\n",
    "def mod_MaP_inputs(X, y1):\n",
    "\n",
    "    # Used for the Move-a-Piece NN. Adds the selected piece (y1 target) as an input to the second NN\n",
    "    #\n",
    "    # argument : SaP input array\n",
    "    # return   : MaP input Array (72 elements per row)\n",
    "    \n",
    "    MaP_input = []\n",
    "    for i in range(len(X)):\n",
    "        MaP_input.append(np.append(X[i], y1[i]))\n",
    "    return np.array(MaP_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73cf9343-d32e-4537-a5e1-3f9835b55fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgn_games = read_pgn_from_file(\"fics_games/fics_2020.pgn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "58b4cef4-21e3-4305-84c7-dbded137565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract inputs from a list of PGN games\n",
    "game_num = len(pgn_games)\n",
    "\n",
    "X1_white_early, y1_white_early, y2_white_early = conv_nn_data(pgn_games[:game_num], color = True, period = 'early')\n",
    "X1_white_mid, y1_white_mid, y2_white_mid = conv_nn_data(pgn_games[:game_num], color = True, period = 'mid')\n",
    "X1_white_late, y1_white_late, y2_white_late = conv_nn_data(pgn_games[:game_num], color = True, period = 'late')\n",
    "\n",
    "X2_white_early = mod_MaP_inputs(X1_white_early, y1_white_early)\n",
    "X2_white_mid = mod_MaP_inputs(X1_white_mid, y1_white_mid)\n",
    "X2_white_late = mod_MaP_inputs(X1_white_late, y1_white_late)\n",
    "\n",
    "# convert the targets to 64-element arrays to contain the decision space of the board (8 x 8 squares)\n",
    "y1_white_early = tf.keras.utils.to_categorical(y1_white_early, num_classes=64)\n",
    "y2_white_early = tf.keras.utils.to_categorical(y2_white_early, num_classes=64)\n",
    "\n",
    "y1_white_mid= tf.keras.utils.to_categorical(y1_white_mid, num_classes=64)\n",
    "y2_white_mid = tf.keras.utils.to_categorical(y2_white_mid, num_classes=64)\n",
    "\n",
    "y1_white_late = tf.keras.utils.to_categorical(y1_white_late, num_classes=64)\n",
    "y2_white_late = tf.keras.utils.to_categorical(y2_white_late, num_classes=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ca513758-cadf-4587-83bb-db2a1213a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "SaP_model = keras.Sequential([\n",
    "    keras.layers.Dense(units=1024, input_shape=(71, ), activation=\"relu\", name=\"hidden_layer1\"),\n",
    "    keras.layers.Dropout(0.2),\n",
    "#     keras.layers.Dense(units=512, activation=\"relu\", name=\"hidden_layer2\"),\n",
    "#     keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(units=64, activation=\"softmax\", name=\"output_layer\")\n",
    "])\n",
    "\n",
    "SaP_model.compile(optimizer=\"nadam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "MaP_model = keras.Sequential([\n",
    "    keras.layers.Dense(units=1024, input_shape=(72, ), activation=\"relu\", name=\"hidden_layer1\"),\n",
    "    keras.layers.Dropout(0.2),\n",
    "#     keras.layers.Dense(units=512, activation=\"relu\", name=\"hidden_layer2\"),\n",
    "#     keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(units=64, activation=\"softmax\", name=\"output_layer\")\n",
    "])\n",
    "\n",
    "MaP_model.compile(optimizer=\"nadam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3cc19bc9-2113-47f7-8766-2c31277caad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SaP_white_early, SaP_white_mid, SaP_white_late, SaP_black_early, SaP_black_mid, SaP_white_late = SaP_model, SaP_model, SaP_model, SaP_model, SaP_model, SaP_model\n",
    "MaP_white_early, MaP_white_mid, MaP_white_late, MaP_black_early, MaP_black_mid, MaP_white_late = MaP_model, MaP_model, MaP_model, MaP_model, MaP_model, MaP_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9d40edf8-92d8-424f-ad88-cce144b0a64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.6908 - acc: 0.4690 - val_loss: 1.4592 - val_acc: 0.5312\n",
      "Epoch 2/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.4760 - acc: 0.5273 - val_loss: 1.4143 - val_acc: 0.5386\n",
      "Epoch 3/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.4193 - acc: 0.5428 - val_loss: 1.3727 - val_acc: 0.5553\n",
      "Epoch 4/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.3873 - acc: 0.5515 - val_loss: 1.3666 - val_acc: 0.5541\n",
      "Epoch 5/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.3674 - acc: 0.5577 - val_loss: 1.3545 - val_acc: 0.5583\n",
      "Epoch 6/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.3504 - acc: 0.5628 - val_loss: 1.3632 - val_acc: 0.5647\n",
      "Epoch 7/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.3349 - acc: 0.5665 - val_loss: 1.3465 - val_acc: 0.5646\n",
      "Epoch 8/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.3241 - acc: 0.5703 - val_loss: 1.3499 - val_acc: 0.5644\n",
      "Epoch 9/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.3143 - acc: 0.5719 - val_loss: 1.3613 - val_acc: 0.5638\n",
      "Epoch 10/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.3063 - acc: 0.5744 - val_loss: 1.3416 - val_acc: 0.5705\n",
      "Epoch 11/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.3021 - acc: 0.5766 - val_loss: 1.3524 - val_acc: 0.5708\n",
      "Epoch 12/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.2948 - acc: 0.5768 - val_loss: 1.3599 - val_acc: 0.5735\n",
      "Epoch 13/50\n",
      "8117/8118 [============================>.] - ETA: 0s - loss: 1.2915 - acc: 0.5794\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.2915 - acc: 0.5794 - val_loss: 1.3583 - val_acc: 0.5709\n",
      "Epoch 14/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.2267 - acc: 0.5942 - val_loss: 1.3206 - val_acc: 0.5823\n",
      "Epoch 15/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.2126 - acc: 0.5983 - val_loss: 1.3251 - val_acc: 0.5854\n",
      "Epoch 16/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.2075 - acc: 0.5990 - val_loss: 1.3256 - val_acc: 0.5843\n",
      "Epoch 17/50\n",
      "8104/8118 [============================>.] - ETA: 0s - loss: 1.2024 - acc: 0.6012\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.2025 - acc: 0.6012 - val_loss: 1.3310 - val_acc: 0.5830\n",
      "Epoch 18/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.1695 - acc: 0.6088 - val_loss: 1.3142 - val_acc: 0.5894\n",
      "Epoch 19/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.1612 - acc: 0.6108 - val_loss: 1.3164 - val_acc: 0.5903\n",
      "Epoch 20/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.1590 - acc: 0.6117 - val_loss: 1.3184 - val_acc: 0.5881\n",
      "Epoch 21/50\n",
      "8091/8118 [============================>.] - ETA: 0s - loss: 1.1554 - acc: 0.6132\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.1555 - acc: 0.6132 - val_loss: 1.3198 - val_acc: 0.5920\n",
      "Epoch 22/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.1362 - acc: 0.6179 - val_loss: 1.3112 - val_acc: 0.5936\n",
      "Epoch 23/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.1339 - acc: 0.6186 - val_loss: 1.3109 - val_acc: 0.5931\n",
      "Epoch 24/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.1323 - acc: 0.6190 - val_loss: 1.3156 - val_acc: 0.5927\n",
      "Epoch 25/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.1308 - acc: 0.6193 - val_loss: 1.3155 - val_acc: 0.5928\n",
      "Epoch 26/50\n",
      "8104/8118 [============================>.] - ETA: 0s - loss: 1.1296 - acc: 0.6193\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.1296 - acc: 0.6193 - val_loss: 1.3151 - val_acc: 0.5912\n",
      "Epoch 27/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.1190 - acc: 0.6219 - val_loss: 1.3146 - val_acc: 0.5941\n",
      "Epoch 28/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.1180 - acc: 0.6222 - val_loss: 1.3125 - val_acc: 0.5936\n",
      "Epoch 29/50\n",
      "8095/8118 [============================>.] - ETA: 0s - loss: 1.1160 - acc: 0.6233\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.1160 - acc: 0.6233 - val_loss: 1.3146 - val_acc: 0.5932\n",
      "Epoch 30/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.1118 - acc: 0.6250 - val_loss: 1.3127 - val_acc: 0.5930\n",
      "Epoch 31/50\n",
      "8117/8118 [============================>.] - ETA: 0s - loss: 1.1104 - acc: 0.6247Restoring model weights from the end of the best epoch.\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 1.1104 - acc: 0.6248 - val_loss: 1.3135 - val_acc: 0.5932\n",
      "Epoch 00031: early stopping\n",
      "Epoch 1/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 6.2365 - acc: 0.1565 - val_loss: 4.2960 - val_acc: 0.1809\n",
      "Epoch 2/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 4.2512 - acc: 0.1703 - val_loss: 3.5546 - val_acc: 0.2047\n",
      "Epoch 3/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.6466 - acc: 0.1922 - val_loss: 3.2062 - val_acc: 0.2219\n",
      "Epoch 4/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.3252 - acc: 0.2071 - val_loss: 3.0076 - val_acc: 0.2355\n",
      "Epoch 5/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.1298 - acc: 0.2186 - val_loss: 2.8800 - val_acc: 0.2453\n",
      "Epoch 6/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.9957 - acc: 0.2271 - val_loss: 2.7904 - val_acc: 0.2518\n",
      "Epoch 7/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.9064 - acc: 0.2339 - val_loss: 2.7239 - val_acc: 0.2573\n",
      "Epoch 8/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.8321 - acc: 0.2385 - val_loss: 2.6690 - val_acc: 0.2621\n",
      "Epoch 9/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.7732 - acc: 0.2442 - val_loss: 2.6238 - val_acc: 0.2658\n",
      "Epoch 10/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.7227 - acc: 0.2490 - val_loss: 2.5878 - val_acc: 0.2692\n",
      "Epoch 11/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.6815 - acc: 0.2533 - val_loss: 2.5541 - val_acc: 0.2744\n",
      "Epoch 12/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.6442 - acc: 0.2564 - val_loss: 2.5255 - val_acc: 0.2774\n",
      "Epoch 13/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.6110 - acc: 0.2595 - val_loss: 2.5001 - val_acc: 0.2792\n",
      "Epoch 14/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.5819 - acc: 0.2641 - val_loss: 2.4765 - val_acc: 0.2825\n",
      "Epoch 15/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.5557 - acc: 0.2670 - val_loss: 2.4552 - val_acc: 0.2867\n",
      "Epoch 16/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.5298 - acc: 0.2698 - val_loss: 2.4362 - val_acc: 0.2903\n",
      "Epoch 17/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.5126 - acc: 0.2729 - val_loss: 2.4200 - val_acc: 0.2924\n",
      "Epoch 18/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.4900 - acc: 0.2748 - val_loss: 2.4033 - val_acc: 0.2952\n",
      "Epoch 19/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.4703 - acc: 0.2778 - val_loss: 2.3892 - val_acc: 0.2968\n",
      "Epoch 20/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.4553 - acc: 0.2807 - val_loss: 2.3752 - val_acc: 0.2982\n",
      "Epoch 21/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.4369 - acc: 0.2817 - val_loss: 2.3629 - val_acc: 0.3014\n",
      "Epoch 22/50\n",
      "12093/12093 [==============================] - 27s 2ms/step - loss: 2.4233 - acc: 0.2838 - val_loss: 2.3501 - val_acc: 0.3013\n",
      "Epoch 23/50\n",
      "12093/12093 [==============================] - 27s 2ms/step - loss: 2.4088 - acc: 0.2857 - val_loss: 2.3392 - val_acc: 0.3035\n",
      "Epoch 24/50\n",
      "12093/12093 [==============================] - 27s 2ms/step - loss: 2.3955 - acc: 0.2884 - val_loss: 2.3290 - val_acc: 0.3039\n",
      "Epoch 25/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.3827 - acc: 0.2890 - val_loss: 2.3189 - val_acc: 0.3060\n",
      "Epoch 26/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.3708 - acc: 0.2907 - val_loss: 2.3092 - val_acc: 0.3071\n",
      "Epoch 27/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.3588 - acc: 0.2916 - val_loss: 2.2998 - val_acc: 0.3081\n",
      "Epoch 28/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.3491 - acc: 0.2942 - val_loss: 2.2908 - val_acc: 0.3097\n",
      "Epoch 29/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.3375 - acc: 0.2953 - val_loss: 2.2819 - val_acc: 0.3106\n",
      "Epoch 30/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.3265 - acc: 0.2966 - val_loss: 2.2735 - val_acc: 0.3110\n",
      "Epoch 31/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.3174 - acc: 0.2979 - val_loss: 2.2662 - val_acc: 0.3117\n",
      "Epoch 32/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.3075 - acc: 0.2999 - val_loss: 2.2581 - val_acc: 0.3128\n",
      "Epoch 33/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.3008 - acc: 0.2994 - val_loss: 2.2514 - val_acc: 0.3140\n",
      "Epoch 34/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.2902 - acc: 0.3015 - val_loss: 2.2446 - val_acc: 0.3151\n",
      "Epoch 35/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.2830 - acc: 0.3028 - val_loss: 2.2380 - val_acc: 0.3158\n",
      "Epoch 36/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.2748 - acc: 0.3037 - val_loss: 2.2319 - val_acc: 0.3180\n",
      "Epoch 37/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.2661 - acc: 0.3051 - val_loss: 2.2258 - val_acc: 0.3172\n",
      "Epoch 38/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.2603 - acc: 0.3063 - val_loss: 2.2213 - val_acc: 0.3193\n",
      "Epoch 39/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.2538 - acc: 0.3072 - val_loss: 2.2155 - val_acc: 0.3206\n",
      "Epoch 40/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.2469 - acc: 0.3073 - val_loss: 2.2099 - val_acc: 0.3209\n",
      "Epoch 41/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.2402 - acc: 0.3091 - val_loss: 2.2044 - val_acc: 0.3212\n",
      "Epoch 42/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.2344 - acc: 0.3097 - val_loss: 2.2006 - val_acc: 0.3214\n",
      "Epoch 43/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.2288 - acc: 0.3096 - val_loss: 2.1967 - val_acc: 0.3237\n",
      "Epoch 44/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.2228 - acc: 0.3111 - val_loss: 2.1913 - val_acc: 0.3233\n",
      "Epoch 45/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.2154 - acc: 0.3126 - val_loss: 2.1872 - val_acc: 0.3266\n",
      "Epoch 46/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.2114 - acc: 0.3123 - val_loss: 2.1829 - val_acc: 0.3266\n",
      "Epoch 47/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.2050 - acc: 0.3136 - val_loss: 2.1794 - val_acc: 0.3262\n",
      "Epoch 48/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 2.2032 - acc: 0.3145 - val_loss: 2.1759 - val_acc: 0.3284\n",
      "Epoch 49/50\n",
      "12093/12093 [==============================] - 27s 2ms/step - loss: 2.1977 - acc: 0.3142 - val_loss: 2.1720 - val_acc: 0.3288\n",
      "Epoch 50/50\n",
      "12093/12093 [==============================] - 27s 2ms/step - loss: 2.1915 - acc: 0.3156 - val_loss: 2.1690 - val_acc: 0.3281\n",
      "Epoch 1/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 2.6147 - acc: 0.3195 - val_loss: 2.2107 - val_acc: 0.3420\n",
      "Epoch 2/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 2.3411 - acc: 0.3368 - val_loss: 2.1040 - val_acc: 0.3477\n",
      "Epoch 3/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 2.2431 - acc: 0.3454 - val_loss: 2.0520 - val_acc: 0.3524\n",
      "Epoch 4/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 2.1871 - acc: 0.3499 - val_loss: 2.0174 - val_acc: 0.3562\n",
      "Epoch 5/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 2.1465 - acc: 0.3541 - val_loss: 1.9918 - val_acc: 0.3587\n",
      "Epoch 6/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 2.1181 - acc: 0.3570 - val_loss: 1.9704 - val_acc: 0.3602\n",
      "Epoch 7/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 2.0914 - acc: 0.3597 - val_loss: 1.9559 - val_acc: 0.3612\n",
      "Epoch 8/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 2.0733 - acc: 0.3615 - val_loss: 1.9393 - val_acc: 0.3639\n",
      "Epoch 9/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 2.0572 - acc: 0.3633 - val_loss: 1.9271 - val_acc: 0.3636\n",
      "Epoch 10/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 2.0426 - acc: 0.3637 - val_loss: 1.9152 - val_acc: 0.3651\n",
      "Epoch 11/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 2.0273 - acc: 0.3655 - val_loss: 1.9043 - val_acc: 0.3658\n",
      "Epoch 12/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 2.0155 - acc: 0.3668 - val_loss: 1.8944 - val_acc: 0.3682\n",
      "Epoch 13/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 2.0046 - acc: 0.3683 - val_loss: 1.8855 - val_acc: 0.3686\n",
      "Epoch 14/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.9937 - acc: 0.3700 - val_loss: 1.8773 - val_acc: 0.3701\n",
      "Epoch 15/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.9848 - acc: 0.3697 - val_loss: 1.8690 - val_acc: 0.3704\n",
      "Epoch 16/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.9758 - acc: 0.3709 - val_loss: 1.8619 - val_acc: 0.3699\n",
      "Epoch 17/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.9670 - acc: 0.3722 - val_loss: 1.8541 - val_acc: 0.3723\n",
      "Epoch 18/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.9581 - acc: 0.3729 - val_loss: 1.8467 - val_acc: 0.3727\n",
      "Epoch 19/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.9510 - acc: 0.3732 - val_loss: 1.8387 - val_acc: 0.3727\n",
      "Epoch 20/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.9438 - acc: 0.3743 - val_loss: 1.8321 - val_acc: 0.3753\n",
      "Epoch 21/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.9366 - acc: 0.3747 - val_loss: 1.8287 - val_acc: 0.3739\n",
      "Epoch 22/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.9297 - acc: 0.3750 - val_loss: 1.8208 - val_acc: 0.3743\n",
      "Epoch 23/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 1.9250 - acc: 0.3754 - val_loss: 1.8150 - val_acc: 0.3770\n",
      "Epoch 24/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 1.9182 - acc: 0.3756 - val_loss: 1.8085 - val_acc: 0.3770\n",
      "Epoch 25/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 1.9124 - acc: 0.3761 - val_loss: 1.8034 - val_acc: 0.3779\n",
      "Epoch 26/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 1.9060 - acc: 0.3775 - val_loss: 1.8001 - val_acc: 0.3770\n",
      "Epoch 27/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 1.9011 - acc: 0.3774 - val_loss: 1.7962 - val_acc: 0.3770\n",
      "Epoch 28/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8968 - acc: 0.3780 - val_loss: 1.7899 - val_acc: 0.3785\n",
      "Epoch 29/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8918 - acc: 0.3788 - val_loss: 1.7863 - val_acc: 0.3802\n",
      "Epoch 30/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8876 - acc: 0.3793 - val_loss: 1.7820 - val_acc: 0.3798\n",
      "Epoch 31/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8823 - acc: 0.3795 - val_loss: 1.7784 - val_acc: 0.3810\n",
      "Epoch 32/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8787 - acc: 0.3799 - val_loss: 1.7753 - val_acc: 0.3803\n",
      "Epoch 33/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8742 - acc: 0.3804 - val_loss: 1.7709 - val_acc: 0.3813\n",
      "Epoch 34/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8709 - acc: 0.3809 - val_loss: 1.7687 - val_acc: 0.3814\n",
      "Epoch 35/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8677 - acc: 0.3802 - val_loss: 1.7665 - val_acc: 0.3816\n",
      "Epoch 36/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8622 - acc: 0.3814 - val_loss: 1.7621 - val_acc: 0.3819\n",
      "Epoch 37/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8592 - acc: 0.3813 - val_loss: 1.7602 - val_acc: 0.3821\n",
      "Epoch 38/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8547 - acc: 0.3822 - val_loss: 1.7570 - val_acc: 0.3827\n",
      "Epoch 39/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8518 - acc: 0.3824 - val_loss: 1.7540 - val_acc: 0.3833\n",
      "Epoch 40/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8480 - acc: 0.3833 - val_loss: 1.7492 - val_acc: 0.3844\n",
      "Epoch 41/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8436 - acc: 0.3832 - val_loss: 1.7480 - val_acc: 0.3833\n",
      "Epoch 42/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8404 - acc: 0.3832 - val_loss: 1.7450 - val_acc: 0.3850\n",
      "Epoch 43/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8372 - acc: 0.3837 - val_loss: 1.7433 - val_acc: 0.3851\n",
      "Epoch 44/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8338 - acc: 0.3841 - val_loss: 1.7404 - val_acc: 0.3850\n",
      "Epoch 45/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8305 - acc: 0.3840 - val_loss: 1.7381 - val_acc: 0.3865\n",
      "Epoch 46/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8286 - acc: 0.3847 - val_loss: 1.7359 - val_acc: 0.3849\n",
      "Epoch 47/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8249 - acc: 0.3846 - val_loss: 1.7329 - val_acc: 0.3859\n",
      "Epoch 48/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8216 - acc: 0.3857 - val_loss: 1.7302 - val_acc: 0.3870\n",
      "Epoch 49/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 1.8186 - acc: 0.3857 - val_loss: 1.7318 - val_acc: 0.3843\n",
      "Epoch 50/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 1.8170 - acc: 0.3861 - val_loss: 1.7284 - val_acc: 0.3866\n",
      "Epoch 1/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 1.1477 - acc: 0.6725 - val_loss: 0.8262 - val_acc: 0.7458\n",
      "Epoch 2/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.8348 - acc: 0.7571 - val_loss: 0.7354 - val_acc: 0.7852\n",
      "Epoch 3/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.7674 - acc: 0.7749 - val_loss: 0.7017 - val_acc: 0.7960\n",
      "Epoch 4/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.7327 - acc: 0.7847 - val_loss: 0.6621 - val_acc: 0.8100\n",
      "Epoch 5/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.7089 - acc: 0.7919 - val_loss: 0.6442 - val_acc: 0.8158\n",
      "Epoch 6/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.6917 - acc: 0.7964 - val_loss: 0.6439 - val_acc: 0.8172\n",
      "Epoch 7/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.6753 - acc: 0.8014 - val_loss: 0.6327 - val_acc: 0.8205\n",
      "Epoch 8/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.6643 - acc: 0.8046 - val_loss: 0.6406 - val_acc: 0.8243\n",
      "Epoch 9/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.6551 - acc: 0.8083 - val_loss: 0.6442 - val_acc: 0.8211\n",
      "Epoch 10/50\n",
      "8112/8118 [============================>.] - ETA: 0s - loss: 0.6456 - acc: 0.8098\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.6455 - acc: 0.8098 - val_loss: 0.6387 - val_acc: 0.8239\n",
      "Epoch 11/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.5643 - acc: 0.8308 - val_loss: 0.5945 - val_acc: 0.8392\n",
      "Epoch 12/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.5467 - acc: 0.8352 - val_loss: 0.5816 - val_acc: 0.8464\n",
      "Epoch 13/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.5396 - acc: 0.8375 - val_loss: 0.5957 - val_acc: 0.8397\n",
      "Epoch 14/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.5304 - acc: 0.8389 - val_loss: 0.5876 - val_acc: 0.8442\n",
      "Epoch 15/50\n",
      "8087/8118 [============================>.] - ETA: 0s - loss: 0.5266 - acc: 0.8399\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.5266 - acc: 0.8399 - val_loss: 0.5925 - val_acc: 0.8443\n",
      "Epoch 16/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4862 - acc: 0.8512 - val_loss: 0.5735 - val_acc: 0.8521\n",
      "Epoch 17/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4791 - acc: 0.8529 - val_loss: 0.5715 - val_acc: 0.8532\n",
      "Epoch 18/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4752 - acc: 0.8534 - val_loss: 0.5736 - val_acc: 0.8528\n",
      "Epoch 19/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4707 - acc: 0.8539 - val_loss: 0.5752 - val_acc: 0.8508\n",
      "Epoch 20/50\n",
      "8090/8118 [============================>.] - ETA: 0s - loss: 0.4679 - acc: 0.8563\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4680 - acc: 0.8563 - val_loss: 0.5741 - val_acc: 0.8549\n",
      "Epoch 21/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4482 - acc: 0.8618 - val_loss: 0.5654 - val_acc: 0.8565\n",
      "Epoch 22/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4449 - acc: 0.8623 - val_loss: 0.5673 - val_acc: 0.8563\n",
      "Epoch 23/50\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 0.4426 - acc: 0.8621 - val_loss: 0.5662 - val_acc: 0.8565\n",
      "Epoch 24/50\n",
      "8103/8118 [============================>.] - ETA: 0s - loss: 0.4408 - acc: 0.8634\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4408 - acc: 0.8634 - val_loss: 0.5659 - val_acc: 0.8576\n",
      "Epoch 25/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4314 - acc: 0.8663 - val_loss: 0.5623 - val_acc: 0.8575\n",
      "Epoch 26/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4257 - acc: 0.8677 - val_loss: 0.5621 - val_acc: 0.8581\n",
      "Epoch 27/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4268 - acc: 0.8664 - val_loss: 0.5620 - val_acc: 0.8584\n",
      "Epoch 28/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4246 - acc: 0.8682 - val_loss: 0.5624 - val_acc: 0.8591\n",
      "Epoch 29/50\n",
      "8118/8118 [==============================] - ETA: 0s - loss: 0.4238 - acc: 0.8675\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4238 - acc: 0.8675 - val_loss: 0.5649 - val_acc: 0.8587\n",
      "Epoch 30/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4186 - acc: 0.8685 - val_loss: 0.5610 - val_acc: 0.8593\n",
      "Epoch 31/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4180 - acc: 0.8692 - val_loss: 0.5616 - val_acc: 0.8595\n",
      "Epoch 32/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4167 - acc: 0.8699 - val_loss: 0.5620 - val_acc: 0.8593\n",
      "Epoch 33/50\n",
      "8098/8118 [============================>.] - ETA: 0s - loss: 0.4145 - acc: 0.8703\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4144 - acc: 0.8704 - val_loss: 0.5624 - val_acc: 0.8588\n",
      "Epoch 34/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4125 - acc: 0.8717 - val_loss: 0.5614 - val_acc: 0.8598\n",
      "Epoch 35/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4122 - acc: 0.8708 - val_loss: 0.5612 - val_acc: 0.8594\n",
      "Epoch 36/50\n",
      "8090/8118 [============================>.] - ETA: 0s - loss: 0.4134 - acc: 0.8709\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4134 - acc: 0.8709 - val_loss: 0.5609 - val_acc: 0.8595\n",
      "Epoch 37/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4097 - acc: 0.8716 - val_loss: 0.5607 - val_acc: 0.8593\n",
      "Epoch 38/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4106 - acc: 0.8715 - val_loss: 0.5607 - val_acc: 0.8597\n",
      "Epoch 39/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4107 - acc: 0.8723 - val_loss: 0.5610 - val_acc: 0.8595\n",
      "Epoch 40/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4098 - acc: 0.8714 - val_loss: 0.5604 - val_acc: 0.8597\n",
      "Epoch 41/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4098 - acc: 0.8711 - val_loss: 0.5608 - val_acc: 0.8598\n",
      "Epoch 42/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4101 - acc: 0.8713 - val_loss: 0.5609 - val_acc: 0.8596\n",
      "Epoch 43/50\n",
      "8087/8118 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.8719\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "8118/8118 [==============================] - 14s 2ms/step - loss: 0.4095 - acc: 0.8719 - val_loss: 0.5605 - val_acc: 0.8594\n",
      "Epoch 44/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4104 - acc: 0.8715 - val_loss: 0.5610 - val_acc: 0.8595\n",
      "Epoch 45/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4093 - acc: 0.8717 - val_loss: 0.5609 - val_acc: 0.8597\n",
      "Epoch 46/50\n",
      "8096/8118 [============================>.] - ETA: 0s - loss: 0.4107 - acc: 0.8714\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4109 - acc: 0.8714 - val_loss: 0.5606 - val_acc: 0.8595\n",
      "Epoch 47/50\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4095 - acc: 0.8720 - val_loss: 0.5607 - val_acc: 0.8595\n",
      "Epoch 48/50\n",
      "8103/8118 [============================>.] - ETA: 0s - loss: 0.4099 - acc: 0.8722Restoring model weights from the end of the best epoch.\n",
      "8118/8118 [==============================] - 13s 2ms/step - loss: 0.4098 - acc: 0.8722 - val_loss: 0.5607 - val_acc: 0.8597\n",
      "Epoch 00048: early stopping\n",
      "Epoch 1/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 11.3367 - acc: 0.2127 - val_loss: 9.3464 - val_acc: 0.2357\n",
      "Epoch 2/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 9.7156 - acc: 0.2060 - val_loss: 8.1709 - val_acc: 0.2300\n",
      "Epoch 3/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 8.6676 - acc: 0.1989 - val_loss: 7.3880 - val_acc: 0.2223\n",
      "Epoch 4/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 7.9499 - acc: 0.1917 - val_loss: 6.8176 - val_acc: 0.2172\n",
      "Epoch 5/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 7.3760 - acc: 0.1871 - val_loss: 6.3783 - val_acc: 0.2120\n",
      "Epoch 6/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 6.9255 - acc: 0.1818 - val_loss: 6.0273 - val_acc: 0.2079\n",
      "Epoch 7/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 6.5609 - acc: 0.1798 - val_loss: 5.7385 - val_acc: 0.2055\n",
      "Epoch 8/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 6.2551 - acc: 0.1764 - val_loss: 5.4955 - val_acc: 0.2027\n",
      "Epoch 9/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 6.0050 - acc: 0.1749 - val_loss: 5.2879 - val_acc: 0.2006\n",
      "Epoch 10/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 5.7781 - acc: 0.1721 - val_loss: 5.1068 - val_acc: 0.1992\n",
      "Epoch 11/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 5.5713 - acc: 0.1712 - val_loss: 4.9472 - val_acc: 0.1983\n",
      "Epoch 12/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 5.3956 - acc: 0.1706 - val_loss: 4.8056 - val_acc: 0.1985\n",
      "Epoch 13/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 5.2380 - acc: 0.1701 - val_loss: 4.6793 - val_acc: 0.1985\n",
      "Epoch 14/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 5.0913 - acc: 0.1705 - val_loss: 4.5662 - val_acc: 0.1986\n",
      "Epoch 15/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 4.9629 - acc: 0.1699 - val_loss: 4.4645 - val_acc: 0.1983\n",
      "Epoch 16/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 4.8407 - acc: 0.1699 - val_loss: 4.3735 - val_acc: 0.1977\n",
      "Epoch 17/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 4.7395 - acc: 0.1701 - val_loss: 4.2907 - val_acc: 0.1991\n",
      "Epoch 18/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 4.6460 - acc: 0.1708 - val_loss: 4.2157 - val_acc: 0.2006\n",
      "Epoch 19/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 4.5553 - acc: 0.1711 - val_loss: 4.1471 - val_acc: 0.2026\n",
      "Epoch 20/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 4.4771 - acc: 0.1713 - val_loss: 4.0841 - val_acc: 0.2038\n",
      "Epoch 21/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 4.3975 - acc: 0.1715 - val_loss: 4.0266 - val_acc: 0.2040\n",
      "Epoch 22/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 4.3288 - acc: 0.1726 - val_loss: 3.9739 - val_acc: 0.2044\n",
      "Epoch 23/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 4.2733 - acc: 0.1718 - val_loss: 3.9257 - val_acc: 0.2051\n",
      "Epoch 24/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 4.2140 - acc: 0.1735 - val_loss: 3.8817 - val_acc: 0.2051\n",
      "Epoch 25/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 4.1630 - acc: 0.1747 - val_loss: 3.8410 - val_acc: 0.2061\n",
      "Epoch 26/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 4.1169 - acc: 0.1746 - val_loss: 3.8036 - val_acc: 0.2062\n",
      "Epoch 27/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 4.0651 - acc: 0.1751 - val_loss: 3.7688 - val_acc: 0.2067\n",
      "Epoch 28/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 4.0224 - acc: 0.1760 - val_loss: 3.7359 - val_acc: 0.2058\n",
      "Epoch 29/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.9841 - acc: 0.1766 - val_loss: 3.7052 - val_acc: 0.2061\n",
      "Epoch 30/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.9455 - acc: 0.1778 - val_loss: 3.6760 - val_acc: 0.2069\n",
      "Epoch 31/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.9135 - acc: 0.1786 - val_loss: 3.6486 - val_acc: 0.2070\n",
      "Epoch 32/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.8859 - acc: 0.1788 - val_loss: 3.6227 - val_acc: 0.2078\n",
      "Epoch 33/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.8508 - acc: 0.1798 - val_loss: 3.5979 - val_acc: 0.2083\n",
      "Epoch 34/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.8205 - acc: 0.1810 - val_loss: 3.5741 - val_acc: 0.2102\n",
      "Epoch 35/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.7883 - acc: 0.1818 - val_loss: 3.5516 - val_acc: 0.2113\n",
      "Epoch 36/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.7672 - acc: 0.1825 - val_loss: 3.5302 - val_acc: 0.2121\n",
      "Epoch 37/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.7401 - acc: 0.1828 - val_loss: 3.5097 - val_acc: 0.2132\n",
      "Epoch 38/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.7190 - acc: 0.1841 - val_loss: 3.4900 - val_acc: 0.2151\n",
      "Epoch 39/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.6933 - acc: 0.1847 - val_loss: 3.4709 - val_acc: 0.2173\n",
      "Epoch 40/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.6738 - acc: 0.1863 - val_loss: 3.4528 - val_acc: 0.2183\n",
      "Epoch 41/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.6494 - acc: 0.1880 - val_loss: 3.4353 - val_acc: 0.2191\n",
      "Epoch 42/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.6311 - acc: 0.1887 - val_loss: 3.4181 - val_acc: 0.2208\n",
      "Epoch 43/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.6052 - acc: 0.1894 - val_loss: 3.4016 - val_acc: 0.2218\n",
      "Epoch 44/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.5856 - acc: 0.1910 - val_loss: 3.3854 - val_acc: 0.2239\n",
      "Epoch 45/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.5713 - acc: 0.1925 - val_loss: 3.3699 - val_acc: 0.2252\n",
      "Epoch 46/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.5529 - acc: 0.1943 - val_loss: 3.3550 - val_acc: 0.2262\n",
      "Epoch 47/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.5342 - acc: 0.1950 - val_loss: 3.3407 - val_acc: 0.2272\n",
      "Epoch 48/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.5195 - acc: 0.1963 - val_loss: 3.3266 - val_acc: 0.2285\n",
      "Epoch 49/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.5034 - acc: 0.1971 - val_loss: 3.3131 - val_acc: 0.2296\n",
      "Epoch 50/50\n",
      "12093/12093 [==============================] - 26s 2ms/step - loss: 3.4875 - acc: 0.1981 - val_loss: 3.3000 - val_acc: 0.2318\n",
      "Epoch 1/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 5.1833 - acc: 0.0491 - val_loss: 4.5605 - val_acc: 0.0551\n",
      "Epoch 2/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 4.6556 - acc: 0.0487 - val_loss: 4.3827 - val_acc: 0.0561\n",
      "Epoch 3/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 4.4794 - acc: 0.0520 - val_loss: 4.2649 - val_acc: 0.0611\n",
      "Epoch 4/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 4.3587 - acc: 0.0556 - val_loss: 4.1760 - val_acc: 0.0665\n",
      "Epoch 5/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 4.2655 - acc: 0.0592 - val_loss: 4.1059 - val_acc: 0.0697\n",
      "Epoch 6/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 4.1933 - acc: 0.0620 - val_loss: 4.0483 - val_acc: 0.0732\n",
      "Epoch 7/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 4.1313 - acc: 0.0644 - val_loss: 4.0000 - val_acc: 0.0757\n",
      "Epoch 8/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 4.0826 - acc: 0.0670 - val_loss: 3.9583 - val_acc: 0.0785\n",
      "Epoch 9/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 4.0387 - acc: 0.0686 - val_loss: 3.9223 - val_acc: 0.0808\n",
      "Epoch 10/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.9966 - acc: 0.0708 - val_loss: 3.8904 - val_acc: 0.0828\n",
      "Epoch 11/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 3.9654 - acc: 0.0726 - val_loss: 3.8617 - val_acc: 0.0848\n",
      "Epoch 12/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 3.9359 - acc: 0.0747 - val_loss: 3.8358 - val_acc: 0.0866\n",
      "Epoch 13/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 3.9079 - acc: 0.0762 - val_loss: 3.8124 - val_acc: 0.0885\n",
      "Epoch 14/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.8831 - acc: 0.0779 - val_loss: 3.7910 - val_acc: 0.0902\n",
      "Epoch 15/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.8602 - acc: 0.0797 - val_loss: 3.7713 - val_acc: 0.0921\n",
      "Epoch 16/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.8395 - acc: 0.0812 - val_loss: 3.7533 - val_acc: 0.0936\n",
      "Epoch 17/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.8218 - acc: 0.0826 - val_loss: 3.7364 - val_acc: 0.0949\n",
      "Epoch 18/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 3.8042 - acc: 0.0839 - val_loss: 3.7210 - val_acc: 0.0961\n",
      "Epoch 19/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 3.7883 - acc: 0.0853 - val_loss: 3.7066 - val_acc: 0.0970\n",
      "Epoch 20/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 3.7734 - acc: 0.0860 - val_loss: 3.6933 - val_acc: 0.0976\n",
      "Epoch 21/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.7589 - acc: 0.0876 - val_loss: 3.6804 - val_acc: 0.0988\n",
      "Epoch 22/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 3.7453 - acc: 0.0884 - val_loss: 3.6685 - val_acc: 0.0998\n",
      "Epoch 23/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.7336 - acc: 0.0893 - val_loss: 3.6573 - val_acc: 0.1006\n",
      "Epoch 24/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.7218 - acc: 0.0903 - val_loss: 3.6466 - val_acc: 0.1011\n",
      "Epoch 25/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.7097 - acc: 0.0905 - val_loss: 3.6366 - val_acc: 0.1021\n",
      "Epoch 26/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 3.6997 - acc: 0.0916 - val_loss: 3.6269 - val_acc: 0.1024\n",
      "Epoch 27/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.6897 - acc: 0.0927 - val_loss: 3.6180 - val_acc: 0.1032\n",
      "Epoch 28/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.6803 - acc: 0.0935 - val_loss: 3.6092 - val_acc: 0.1038\n",
      "Epoch 29/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 3.6730 - acc: 0.0943 - val_loss: 3.6010 - val_acc: 0.1051\n",
      "Epoch 30/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.6641 - acc: 0.0949 - val_loss: 3.5933 - val_acc: 0.1061\n",
      "Epoch 31/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.6550 - acc: 0.0958 - val_loss: 3.5854 - val_acc: 0.1068\n",
      "Epoch 32/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.6482 - acc: 0.0961 - val_loss: 3.5783 - val_acc: 0.1075\n",
      "Epoch 33/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.6410 - acc: 0.0970 - val_loss: 3.5714 - val_acc: 0.1085\n",
      "Epoch 34/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 3.6325 - acc: 0.0972 - val_loss: 3.5647 - val_acc: 0.1094\n",
      "Epoch 35/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 3.6262 - acc: 0.0982 - val_loss: 3.5582 - val_acc: 0.1097\n",
      "Epoch 36/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.6208 - acc: 0.0987 - val_loss: 3.5521 - val_acc: 0.1104\n",
      "Epoch 37/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.6126 - acc: 0.0996 - val_loss: 3.5462 - val_acc: 0.1116\n",
      "Epoch 38/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.6068 - acc: 0.1003 - val_loss: 3.5404 - val_acc: 0.1122\n",
      "Epoch 39/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 3.6018 - acc: 0.1008 - val_loss: 3.5348 - val_acc: 0.1119\n",
      "Epoch 40/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 3.5963 - acc: 0.1016 - val_loss: 3.5294 - val_acc: 0.1130\n",
      "Epoch 41/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 3.5905 - acc: 0.1012 - val_loss: 3.5242 - val_acc: 0.1133\n",
      "Epoch 42/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.5865 - acc: 0.1019 - val_loss: 3.5191 - val_acc: 0.1137\n",
      "Epoch 43/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.5811 - acc: 0.1027 - val_loss: 3.5142 - val_acc: 0.1140\n",
      "Epoch 44/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.5765 - acc: 0.1033 - val_loss: 3.5092 - val_acc: 0.1146\n",
      "Epoch 45/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.5704 - acc: 0.1033 - val_loss: 3.5045 - val_acc: 0.1151\n",
      "Epoch 46/50\n",
      "18681/18681 [==============================] - 40s 2ms/step - loss: 3.5665 - acc: 0.1038 - val_loss: 3.4999 - val_acc: 0.1160\n",
      "Epoch 47/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 3.5620 - acc: 0.1043 - val_loss: 3.4954 - val_acc: 0.1166\n",
      "Epoch 48/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 3.5578 - acc: 0.1049 - val_loss: 3.4913 - val_acc: 0.1168\n",
      "Epoch 49/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 3.5541 - acc: 0.1052 - val_loss: 3.4872 - val_acc: 0.1175\n",
      "Epoch 50/50\n",
      "18681/18681 [==============================] - 41s 2ms/step - loss: 3.5497 - acc: 0.1052 - val_loss: 3.4829 - val_acc: 0.1183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x201a55233d0>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SaP_white_early.fit(x=X1_white_early, y=y1_white_early, epochs=50, validation_split=.25, verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=8, verbose=1, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=.5, patience=3, verbose=1),\n",
    "    ])\n",
    "\n",
    "SaP_white_mid.fit(x=X1_white_mid, y=y1_white_mid, epochs=50, validation_split=.25, verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=8, verbose=1, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=.5, patience=3, verbose=1),\n",
    "    ])\n",
    "\n",
    "SaP_white_late.fit(x=X1_white_late, y=y1_white_late, epochs=50, validation_split=.25, verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=8, verbose=1, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=.5, patience=3, verbose=1),\n",
    "    ])\n",
    "\n",
    "MaP_white_early.fit(x=X2_white_early, y=y2_white_early, epochs=50, validation_split=.25, verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=8, verbose=1, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=.5, patience=3, verbose=1),\n",
    "    ])\n",
    "\n",
    "MaP_white_mid.fit(x=X2_white_mid, y=y2_white_mid, epochs=50, validation_split=.25, verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=8, verbose=1, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=.5, patience=3, verbose=1),\n",
    "    ])\n",
    "\n",
    "MaP_white_late.fit(x=X2_white_late, y=y2_white_late, epochs=50, validation_split=.25, verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=8, verbose=1, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=.5, patience=3, verbose=1),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "f2388a64-4743-430a-b4e7-e7fe5589f5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jakes\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\jakes\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: keras_sequential/SaP_white_early\\assets\n"
     ]
    }
   ],
   "source": [
    "SaP_white_early.save('keras_sequential/SaP_white_early')\n",
    "SaP_white_early.save('keras_sequential/SaP_white_mid')\n",
    "SaP_white_early.save('keras_sequential/SaP_white_late')\n",
    "MaP_white_early.save('keras_sequential/MaP_white_early')\n",
    "MaP_white_early.save('keras_sequential/MaP_white_early')\n",
    "MaP_white_early.save('keras_sequential/MaP_white_early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7f06a036-6577-4125-9f0c-902240016437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_prediction(board):\n",
    "    \n",
    "    input_data = get_board_state(board) + winner\n",
    "    \n",
    "    # Get the initial prediction from the SaP model\n",
    "    if board.ply() <= 14:\n",
    "        from_square_grid = SaP_white_early.predict([input_data]).flatten()\n",
    "    elif 15 <= board.ply() <= 40:\n",
    "        from_square_grid = SaP_white_mid.predict([input_data]).flatten()\n",
    "    else: \n",
    "        from_square_grid = SaP_white_late.predict([input_data]).flatten()\n",
    "        \n",
    "    # Filter that prediction to find the top valid selection\n",
    "    for i in range (63): # go through each of the probabilities found by the NN\n",
    "        valid = False\n",
    "        from_square = np.argmax(from_square_grid)\n",
    "        for j in range(63): # using the found from-square, see if there is a valid move originating from that square\n",
    "            try:\n",
    "                board.find_move(from_square,j)\n",
    "                valid = True\n",
    "                break\n",
    "            except:\n",
    "                from_square_grid[from_square] = 0\n",
    "        if valid == True:\n",
    "            break\n",
    "    \n",
    "    # Get the initial prediction from the MaP model\n",
    "    if board.ply() <= 14:\n",
    "        to_square_grid = MaP_white_early.predict([input_data + [from_square]]).flatten()\n",
    "    elif 15 <= board.ply() <= 40:\n",
    "        to_square_grid = MaP_white_mid.predict([input_data + [from_square]]).flatten()\n",
    "    else: \n",
    "        to_square_grid = MaP_white_late.predict([input_data + [from_square]]).flatten()\n",
    "\n",
    "    # Filter that prediction to find the top valid selection\n",
    "    for i in range (63): # go through each of the probabilities found by the NN\n",
    "        to_square = np.argmax(to_square_grid)\n",
    "        try:\n",
    "            board.find_move(from_square,to_square)\n",
    "            break\n",
    "        except:\n",
    "            to_square_grid[to_square] = 0\n",
    "    \n",
    "    return(board.find_move(from_square, to_square))\n",
    "        \n",
    "# for move in game.mainline_moves():\n",
    "#     input_data = get_board_state(board) + [-1]\n",
    "    \n",
    "#     if board.ply() <= 14:\n",
    "#         prediction = SaP_white_early.predict([input_data])\n",
    "#     elif 15 <= board.ply() <= 40:\n",
    "#         prediction = SaP_white_mid.predict([input_data])\n",
    "#     else: \n",
    "#         prediction = SaP_white_late.predict([input_data])\n",
    "    \n",
    "#     prediction = prediction.reshape(8, 8)\n",
    "#     plt.imshow(prediction, cmap='hot', interpolation='nearest')\n",
    "#     print(\"Turn: \", board.ply(), \"-----------\")\n",
    "#     plt.show()\n",
    "#     print(move.from_square)\n",
    "#     print(np.argmax(prediction))\n",
    "#     print(move)\n",
    "#     board.push(move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9444419f-9015-4398-a47e-2318a89e07a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "no matching legal move for c8b1 (c8 -> b1) in r1b2rk1/pp3ppp/4p3/2pp4/8/2q5/PnP1PPPP/R3KBNR w KQ - 0 16",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-203-e025a57bafc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmove_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#board = chess.Board()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-140-87d15bf7a52d>\u001b[0m in \u001b[0;36mmove_prediction\u001b[1;34m(board)\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mto_square_grid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mto_square\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_move\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrom_square\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_square\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m# for move in game.mainline_moves():\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\chess\\__init__.py\u001b[0m in \u001b[0;36mfind_move\u001b[1;34m(self, from_square, to_square, promotion)\u001b[0m\n\u001b[0;32m   2295\u001b[0m         \u001b[0mmove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_chess960\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchess960\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_square\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_square\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpromotion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2296\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_legal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2297\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"no matching legal move for {move.uci()} ({SQUARE_NAMES[from_square]} -> {SQUARE_NAMES[to_square]}) in {self.fen()}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2299\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmove\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: no matching legal move for c8b1 (c8 -> b1) in r1b2rk1/pp3ppp/4p3/2pp4/8/2q5/PnP1PPPP/R3KBNR w KQ - 0 16"
     ]
    }
   ],
   "source": [
    "move_prediction(board)\n",
    "#board = chess.Board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "ed5d5835-15ab-4310-b4c2-19c79dfece8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" version=\"1.2\" baseProfile=\"tiny\" viewBox=\"0 0 390 390\" width=\"390\" height=\"390\"><defs><g id=\"white-pawn\" class=\"white pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#fff\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"white-knight\" class=\"white knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#000000; stroke:#000000;\" /></g><g id=\"white-bishop\" class=\"white bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#fff\" stroke-linecap=\"butt\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zM15 32c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" /></g><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke-linejoin=\"miter\" /></g><g id=\"white-rook\" class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g id=\"white-king\" class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g id=\"black-pawn\" class=\"black pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"black-knight\" class=\"black knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 24.55,10.4 L 24.1,11.85 L 24.6,12 C 27.75,13 30.25,14.49 32.5,18.75 C 34.75,23.01 35.75,29.06 35.25,39 L 35.2,39.5 L 37.45,39.5 L 37.5,39 C 38,28.94 36.62,22.15 34.25,17.66 C 31.88,13.17 28.46,11.02 25.06,10.5 L 24.55,10.4 z \" style=\"fill:#ececec; stroke:none;\" /></g><g id=\"black-bishop\" class=\"black bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zm6-4c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" fill=\"#000\" stroke-linecap=\"butt\" /><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke=\"#fff\" stroke-linejoin=\"miter\" /></g><g id=\"black-rook\" class=\"black rook\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12.5 32l1.5-2.5h17l1.5 2.5h-20zM12 36v-4h21v4H12z\" stroke-linecap=\"butt\" /><path d=\"M14 29.5v-13h17v13H14z\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M14 16.5L11 14h23l-3 2.5H14zM11 14V9h4v2h5V9h5v2h5V9h4v5H11z\" stroke-linecap=\"butt\" /><path d=\"M12 35.5h21M13 31.5h19M14 29.5h17M14 16.5h17M11 14h23\" fill=\"none\" stroke=\"#fff\" stroke-width=\"1\" stroke-linejoin=\"miter\" /></g><g id=\"black-king\" class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g></defs><rect x=\"0\" y=\"0\" width=\"390\" height=\"390\" fill=\"#212121\" /><g transform=\"translate(20, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(20, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(65, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(65, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(110, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(110, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(155, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(155, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(200, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(200, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(245, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(245, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(290, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(290, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(335, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(335, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(0, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(375, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(0, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(375, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(0, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(375, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(0, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(375, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(0, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(375, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(0, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(375, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(0, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(375, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(0, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><g transform=\"translate(375, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><rect x=\"15\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark a1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"330\" width=\"45\" height=\"45\" class=\"square light lastmove b1\" stroke=\"none\" fill=\"#cdd16a\" /><rect x=\"105\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark c1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"330\" width=\"45\" height=\"45\" class=\"square light d1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark e1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"330\" width=\"45\" height=\"45\" class=\"square light f1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark g1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"330\" width=\"45\" height=\"45\" class=\"square light h1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"285\" width=\"45\" height=\"45\" class=\"square light a2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark b2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"285\" width=\"45\" height=\"45\" class=\"square light c2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark lastmove d2\" stroke=\"none\" fill=\"#aaa23b\" /><rect x=\"195\" y=\"285\" width=\"45\" height=\"45\" class=\"square light e2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark f2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"285\" width=\"45\" height=\"45\" class=\"square light g2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark h2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark a3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"240\" width=\"45\" height=\"45\" class=\"square light b3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark c3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"240\" width=\"45\" height=\"45\" class=\"square light d3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark e3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"240\" width=\"45\" height=\"45\" class=\"square light f3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark g3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"240\" width=\"45\" height=\"45\" class=\"square light h3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"195\" width=\"45\" height=\"45\" class=\"square light a4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark b4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"195\" width=\"45\" height=\"45\" class=\"square light c4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark d4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"195\" width=\"45\" height=\"45\" class=\"square light e4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark f4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"195\" width=\"45\" height=\"45\" class=\"square light g4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark h4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark a5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"150\" width=\"45\" height=\"45\" class=\"square light b5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark c5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"150\" width=\"45\" height=\"45\" class=\"square light d5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark e5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"150\" width=\"45\" height=\"45\" class=\"square light f5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark g5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"150\" width=\"45\" height=\"45\" class=\"square light h5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"105\" width=\"45\" height=\"45\" class=\"square light a6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark b6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"105\" width=\"45\" height=\"45\" class=\"square light c6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark d6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"105\" width=\"45\" height=\"45\" class=\"square light e6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark f6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"105\" width=\"45\" height=\"45\" class=\"square light g6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark h6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark a7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"60\" width=\"45\" height=\"45\" class=\"square light b7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark c7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"60\" width=\"45\" height=\"45\" class=\"square light d7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark e7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"60\" width=\"45\" height=\"45\" class=\"square light f7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark g7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"60\" width=\"45\" height=\"45\" class=\"square light h7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"15\" width=\"45\" height=\"45\" class=\"square light a8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark b8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"15\" width=\"45\" height=\"45\" class=\"square light c8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark d8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"15\" width=\"45\" height=\"45\" class=\"square light e8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark f8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"15\" width=\"45\" height=\"45\" class=\"square light g8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark h8\" stroke=\"none\" fill=\"#d18b47\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(15, 330)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(105, 330)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(240, 330)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(285, 330)\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(330, 330)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(15, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(60, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(105, 285)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(150, 285)\" /><use href=\"#white-king\" xlink:href=\"#white-king\" transform=\"translate(195, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(240, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(285, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(330, 285)\" /><use href=\"#black-bishop\" xlink:href=\"#black-bishop\" transform=\"translate(285, 240)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(150, 150)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(240, 105)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(15, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(60, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(105, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(240, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(285, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(330, 60)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(15, 15)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(60, 15)\" /><use href=\"#black-king\" xlink:href=\"#black-king\" transform=\"translate(195, 15)\" /><use href=\"#black-bishop\" xlink:href=\"#black-bishop\" transform=\"translate(240, 15)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(330, 15)\" /></svg>"
      ],
      "text/plain": [
       "Board('rn2kb1r/ppp2ppp/5n2/3p4/8/6b1/PPPNKPPP/R1B2pNR b kq - 2 12')"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if board.turn == True:\n",
    "    prediction = move_prediction(board)\n",
    "    board.push(prediction)\n",
    "else:\n",
    "    board.push(chess.Move.from_uci(\"c8g3\"))\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "09017e84-4ca4-4a5d-b0b7-97950e60f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "board = chess.Board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3355f519-a520-43f1-ac6e-26576826b9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
